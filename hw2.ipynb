{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "spoken-lottery",
   "metadata": {},
   "source": [
    "Для корректной работы ноутбука необходимо распаковать файл data.zip в ту же директорию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "executed-birmingham",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gensim\n",
    "# !pip install razdel\n",
    "# !pip install pymorphy2\n",
    "# !pip install pyenchant\n",
    "# !pip install python-Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "civilian-treat",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "usual-preserve",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import pymorphy2\n",
    "import itertools\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, roc_auc_score, precision_score\n",
    "from sklearn.metrics import classification_report, precision_recall_curve, confusion_matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from razdel import tokenize\n",
    "from gensim.models import LdaModel\n",
    "from gensim.test.utils import datapath\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "genuine-oklahoma",
   "metadata": {},
   "source": [
    "1. Самостоятельно разобраться с тем, что такое tfidf (документация https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html и еще - https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction)\n",
    "2. Модифицировать код функции get_user_embedding таким образом, чтобы считалось не среднее (как в примере np.mean), а медиана. Применить такое преобразование к данным, обучить модель прогнозирования оттока и посчитать метрики качества и сохранить их: roc auc, precision/recall/f_score (для 3 последних - подобрать оптимальный порог с помощью precision_recall_curve, как это делалось на уроке)\n",
    "3. Повторить п.2, но используя уже не медиану, а max\n",
    "4. (опциональное, если очень хочется) Воспользовавшись полученными знаниями из п.1, повторить пункт 2, но уже взвешивая новости по tfidf (подсказка: нужно получить веса-коэффициенты для каждого документа. Не все документы одинаково информативны и несут какой-то положительный сигнал). Подсказка 2 - нужен именно idf, как вес.\n",
    "5. Сформировать на выходе единую таблицу, сравнивающую качество 3 разных метода получения эмбедингов пользователей: mean, median, max, idf_mean по метрикам roc_auc, precision, recall, f_score\n",
    "6. Сделать самостоятельные выводы и предположения о том, почему тот или иной способ оказался эффективнее остальных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "constant-plymouth",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "waiting-pierce",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER_OF_TOPICS = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "activated-mission",
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_cyr(text: str) -> bool:\n",
    "    \"\"\"\n",
    "    Проверяем, кириллическое ли слово\n",
    "    \"\"\"\n",
    "    return bool(re.search('[а-яА-Я]', text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "regulated-livestock",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    очистка текста\n",
    "\n",
    "    на выходе относительно очищенный текст\n",
    "\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "\n",
    "    text = text.lower()\n",
    "    text = text.strip('\\n').strip('\\r').strip('\\t')\n",
    "    text = re.sub(\"-\\s\\r\\n\\|-\\s\\r\\n|\\r\\n\", '', text)\n",
    "    text = re.sub(r\"[n]\", ' ', text)\n",
    "\n",
    "    text = re.sub(\"[0-9]|[.,:;_%©«»?*!@#№$^•·&()]|[+=]|[[]|[]]|[/]|\", '', text)\n",
    "    text = re.sub(r\"\\r\\n\\t|\\n|\\\\s|\\r\\t|\\\\n\", ' ', text)\n",
    "    text = re.sub(r'[\\xad]|[\\s+]', ' ', text.strip())\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "varied-constant",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(text: str, stopword: list):\n",
    "    \"\"\"\n",
    "    лемматизация\n",
    "        [0] если зашел тип не `str` делаем его `str`\n",
    "        [1] токенизация предложения через razdel\n",
    "        [2] проверка есть ли в начале слова '-'\n",
    "        [3] проверка токена с одного символа\n",
    "        [4] проверка есть ли данное слово в кэше\n",
    "        [5] лемматизация слова\n",
    "        [6] проверка на стоп-слова\n",
    "\n",
    "    на выходе лист отлемматизированых токенов\n",
    "    \"\"\"\n",
    "\n",
    "    cache = {}\n",
    "    morph = pymorphy2.MorphAnalyzer()\n",
    "    # [0]\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "\n",
    "    # [1]\n",
    "    tokens = list(tokenize(text))\n",
    "    words = [_.text for _ in tokens]\n",
    "\n",
    "    words_lem = []\n",
    "    for w in words:\n",
    "        if w[0] == '-':  # [2]\n",
    "            w = w[1:]\n",
    "        if len(w) > 1:  # [3]\n",
    "            if w in cache:  # [4]\n",
    "                words_lem.append(cache[w])\n",
    "            else:  # [5]\n",
    "                temp_cach = cache[w] = morph.parse(w)[0].normal_form\n",
    "                words_lem.append(temp_cach)\n",
    "\n",
    "    words_lem_without_stopwords = [i for i in words_lem if i not in stopword]  # [6]\n",
    "\n",
    "    return words_lem_without_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "experimental-minneapolis",
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_to_list(some_str: str) -> list:\n",
    "    \"\"\"\n",
    "    Функция переводит из строкового типа в лист, отрезая лишние пробелы и кавычки\n",
    "    Потребовалась, так как read_csv из листа сделал строку.\n",
    "    \"\"\"\n",
    "    some_list = some_str.strip('[]').split(',')\n",
    "    return [word.strip().strip(\"''\") for word in some_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "decimal-apple",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lda_vector(text, lda_model):\n",
    "    lda_tuple = lda_model[common_dictionary.doc2bow(text)]\n",
    "    not_null_topics = dict(zip([i[0] for i in lda_tuple], [i[1] for i in lda_tuple]))\n",
    "\n",
    "    output_vector = []\n",
    "    for i in range(NUMBER_OF_TOPICS):\n",
    "        output_vector.append(not_null_topics[i]) if i in not_null_topics else output_vector.append(0)\n",
    "    return np.array(output_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "southwest-librarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "thirty-steel",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_embedding(user_articles_list, doc_dict, idf_dict, func='mean'):\n",
    "    user_articles_list = eval(user_articles_list)\n",
    "    user_vector = np.array([doc_dict[doc_id] for doc_id in user_articles_list])\n",
    "    if func == 'mean':\n",
    "        user_vector = np.mean(user_vector, axis=0)\n",
    "    elif func == 'median':\n",
    "        user_vector = np.median(user_vector, axis=0)\n",
    "    elif func == 'max':\n",
    "        user_vector = np.max(user_vector, axis=0)\n",
    "    elif func == 'idf-mean':\n",
    "        # Скалярно умножим пользовательский вектор на вектор весов idf\n",
    "        idf_vector = np.array([idf_dict[doc_id] for doc_id in user_articles_list])\n",
    "        user_vector = user_vector.T.dot(idf_vector)\n",
    "    else:\n",
    "        raise ValueError(\"func must be from [mean, median, max, idf-mean]\")\n",
    "    return user_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "caring-methodology",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_users(users, topic_matrix, target, func='mean', verbose=True):\n",
    "    \"\"\"\n",
    "    Функция принимает на вход матрицу топиков, целевую переменную и функцию, которая\n",
    "    применяется на вектор пользователей (mean, median, max, idf-mean).\n",
    "    На выходе вектора предсказаний, реальных значений, thr, pr, rec, f1 и индекс лучшего значения\n",
    "    \"\"\"\n",
    "    doc_dict = dict(zip(topic_matrix['doc_id'].values, topic_matrix[[f'topic_{i}' for i in range(NUMBER_OF_TOPICS)]].values))\n",
    "    # Создадим аналогичный предыдущему словарь, содержащий пары значений статья: ее средний idf\n",
    "    idf_dict = dict(zip(topic_matrix['doc_id'].values, topic_matrix['idf_mean'].values))\n",
    "    user_embeddings = pd.DataFrame([i for i in users['articles'].apply(lambda x: get_user_embedding(x, doc_dict, idf_dict, func=func), 1)])\n",
    "\n",
    "    user_embeddings.columns = [f'topic_{i}' for i in range(NUMBER_OF_TOPICS)]\n",
    "\n",
    "    user_embeddings['uid'] = users['uid'].values\n",
    "    user_embeddings = user_embeddings[['uid'] + [f'topic_{i}' for i in range(NUMBER_OF_TOPICS)]]\n",
    "    X = pd.merge(user_embeddings, target, 'left')\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X[[f'topic_{i}' for i in range(NUMBER_OF_TOPICS)]],\n",
    "                                                        X['churn'], random_state=0)\n",
    "    logreg = LogisticRegression()\n",
    "    logreg.fit(X_train, y_train)\n",
    "\n",
    "    y_preds = logreg.predict_proba(X_test)[:, 1]\n",
    "    precision, recall, thresholds = precision_recall_curve(y_test, y_preds)\n",
    "    fscore = (2 * precision * recall) / (precision + recall)\n",
    "    # Так как precision и recall могут быть одновременно нулями - получим nan в массиве.\n",
    "    # Приравняем их к 0\n",
    "    np.nan_to_num(fscore, copy=False)\n",
    "\n",
    "    # locate the index of the largest f score\n",
    "    ix = np.argmax(fscore)\n",
    "    if verbose:\n",
    "        print('Best Threshold=%f, F-Score=%.3f, Precision=%.3f, Recall=%.3f' % (thresholds[ix], fscore[ix],\n",
    "                                                                                precision[ix], recall[ix]))\n",
    "\n",
    "    return y_preds, y_test, thresholds, precision, recall, fscore, ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "sacred-shoulder",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipe_transform(text: str, stopword: list):\n",
    "    \"\"\"\n",
    "    Костыль для последовательной трансформации текста\n",
    "    \"\"\"\n",
    "    return lemmatization(clean_text(text), stopword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "reverse-occupation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def idf_mean(article, com_dict, corpus_len):\n",
    "    \"\"\"\n",
    "    Функция подсчета среднего idf для токенизированного документа. На входе список слов документа,\n",
    "    словарь в формате corpora.dictionary и общее количество документов\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for word in set(article):\n",
    "        result.append(np.log(corpus_len / com_dict.dfs[com_dict.token2id[word]]))\n",
    "    return np.mean(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conventional-toddler",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "geographic-father",
   "metadata": {},
   "outputs": [],
   "source": [
    "news = pd.read_csv(\"articles.csv\")\n",
    "users = pd.read_csv(\"users_articles.csv\")\n",
    "target = pd.read_csv(\"users_churn.csv\")\n",
    "stopwords_ru = stopwords.words('russian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ranging-coupon",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('stopwords.txt') as f:\n",
    "    additional_stopwords = [w.strip() for w in f.readlines() if w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "critical-montgomery",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сделаем небольшую модификацию кода, будем добавлять только русские слова в стоп-список\n",
    "add_words_ru = [word for word in additional_stopwords if has_cyr(word)]\n",
    "stopwords_ru += add_words_ru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "blond-waste",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Так как это добро на моем ноутбуке считалось часа 2, записал в csv\n",
    "# news['title'] = news['title'].apply(lambda x: pipe_transform(x, stopwords_ru))\n",
    "# news.to_csv('./news.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "suffering-steel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Дропаем получившийся после сохранения ненужный стобец с индексами\n",
    "mod_news = pd.read_csv('news.csv').drop('Unnamed: 0', axis=1)\n",
    "\n",
    "# Приводим нужный нам столбец в нормальный вид (из строки в лист)\n",
    "mod_news['title'] = mod_news['title'].apply(lambda x: str_to_list(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "instructional-retention",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_dictionary = Dictionary(mod_news['title'].values)\n",
    "common_corpus = [common_dictionary.doc2bow(text) for text in mod_news['title'].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "objective-simple",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>[заместитель, председатель, правительство, рф,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4896</td>\n",
       "      <td>[матч, финал, кубок, россия, футбол, приостано...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4897</td>\n",
       "      <td>[форвард, авангард, томаш, заборский, прокомме...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4898</td>\n",
       "      <td>[главный, тренер, кубань, юрий, красножанин, п...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4899</td>\n",
       "      <td>[решение, попечительский, совет, владивостокск...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   doc_id                                              title\n",
       "0       6  [заместитель, председатель, правительство, рф,...\n",
       "1    4896  [матч, финал, кубок, россия, футбол, приостано...\n",
       "2    4897  [форвард, авангард, томаш, заборский, прокомме...\n",
       "3    4898  [главный, тренер, кубань, юрий, красножанин, п...\n",
       "4    4899  [решение, попечительский, совет, владивостокск..."
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod_news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "virgin-stable",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Посчитаем вектор средних idf весов для каждого документа\n",
    "idf_vector = [idf_mean(mod_news['title'].iloc[i], common_dictionary, mod_news.shape[0]) \n",
    "              for i in range(mod_news.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "martial-warning",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_file = datapath(\"model.lda\")\n",
    "# Обучили и сохранили. Потом просто вытащим из файла\n",
    "# lda = LdaModel(common_corpus, num_topics=NUMBER_OF_TOPICS, id2word=common_dictionary)\n",
    "# lda.save(lda_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "infrared-electric",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LdaModel.load(lda_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "hearing-folder",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_matrix = pd.DataFrame([get_lda_vector(text, lda) for text in mod_news['title'].values])\n",
    "topic_matrix.columns = [f'topic_{i}' for i in range(NUMBER_OF_TOPICS)]\n",
    "topic_matrix['doc_id'] = mod_news['doc_id'].values\n",
    "topic_matrix = topic_matrix[['doc_id'] + [f'topic_{i}' for i in range(NUMBER_OF_TOPICS)]]\n",
    "topic_matrix['idf_mean'] = idf_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "exotic-university",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>topic_0</th>\n",
       "      <th>topic_1</th>\n",
       "      <th>topic_2</th>\n",
       "      <th>topic_3</th>\n",
       "      <th>topic_4</th>\n",
       "      <th>topic_5</th>\n",
       "      <th>topic_6</th>\n",
       "      <th>topic_7</th>\n",
       "      <th>topic_8</th>\n",
       "      <th>...</th>\n",
       "      <th>topic_16</th>\n",
       "      <th>topic_17</th>\n",
       "      <th>topic_18</th>\n",
       "      <th>topic_19</th>\n",
       "      <th>topic_20</th>\n",
       "      <th>topic_21</th>\n",
       "      <th>topic_22</th>\n",
       "      <th>topic_23</th>\n",
       "      <th>topic_24</th>\n",
       "      <th>idf_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.081637</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.108471</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.082629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4896</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.974720</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.170311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4897</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.703102</td>\n",
       "      <td>0.041559</td>\n",
       "      <td>4.194190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4898</td>\n",
       "      <td>0.101621</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.016286</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.479164</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.824732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4899</td>\n",
       "      <td>0.183356</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.044792</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.419067</td>\n",
       "      <td>0.328033</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.202383</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   doc_id   topic_0  topic_1  topic_2  topic_3   topic_4  topic_5  topic_6  \\\n",
       "0       6  0.000000      0.0      0.0      0.0  0.000000      0.0      0.0   \n",
       "1    4896  0.000000      0.0      0.0      0.0  0.000000      0.0      0.0   \n",
       "2    4897  0.000000      0.0      0.0      0.0  0.000000      0.0      0.0   \n",
       "3    4898  0.101621      0.0      0.0      0.0  0.000000      0.0      0.0   \n",
       "4    4899  0.183356      0.0      0.0      0.0  0.044792      0.0      0.0   \n",
       "\n",
       "    topic_7  topic_8  ...  topic_16  topic_17  topic_18  topic_19  topic_20  \\\n",
       "0  0.000000      0.0  ...       0.0       0.0       0.0       0.0  0.000000   \n",
       "1  0.000000      0.0  ...       0.0       0.0       0.0       0.0  0.000000   \n",
       "2  0.000000      0.0  ...       0.0       0.0       0.0       0.0  0.000000   \n",
       "3  0.016286      0.0  ...       0.0       0.0       0.0       0.0  0.000000   \n",
       "4  0.000000      0.0  ...       0.0       0.0       0.0       0.0  0.419067   \n",
       "\n",
       "   topic_21  topic_22  topic_23  topic_24  idf_mean  \n",
       "0  0.081637       0.0  0.108471  0.000000  4.082629  \n",
       "1  0.000000       0.0  0.974720  0.000000  4.170311  \n",
       "2  0.000000       0.0  0.703102  0.041559  4.194190  \n",
       "3  0.000000       0.0  0.479164  0.000000  3.824732  \n",
       "4  0.328033       0.0  0.000000  0.000000  4.202383  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_matrix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "under-rings",
   "metadata": {},
   "outputs": [],
   "source": [
    "emd_mode = ['mean', 'median', 'max', 'idf-mean']\n",
    "col_names = ['threshhold', 'roc-auc', 'f1-score', 'precision', 'recall']\n",
    "result = []\n",
    "for mode in emd_mode:\n",
    "    y_preds, y_test, thresholds, precision, recall, fscore, ix = embed_users(users, topic_matrix, \n",
    "                                                                             target, func=mode, \n",
    "                                                                             verbose=False)\n",
    "    result.append([thresholds[ix], roc_auc_score(y_test, y_preds), \n",
    "                   fscore[ix], precision[ix], recall[ix]])\n",
    "result_df = pd.DataFrame(result, index=emd_mode, columns=col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "preceding-township",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>threshhold</th>\n",
       "      <th>roc-auc</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.254812</td>\n",
       "      <td>0.944622</td>\n",
       "      <td>0.669159</td>\n",
       "      <td>0.617241</td>\n",
       "      <td>0.730612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>median</th>\n",
       "      <td>0.264692</td>\n",
       "      <td>0.971838</td>\n",
       "      <td>0.781925</td>\n",
       "      <td>0.753788</td>\n",
       "      <td>0.812245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.422737</td>\n",
       "      <td>0.978555</td>\n",
       "      <td>0.806867</td>\n",
       "      <td>0.850679</td>\n",
       "      <td>0.767347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>idf-mean</th>\n",
       "      <td>0.437304</td>\n",
       "      <td>0.973403</td>\n",
       "      <td>0.790419</td>\n",
       "      <td>0.773438</td>\n",
       "      <td>0.808163</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          threshhold   roc-auc  f1-score  precision    recall\n",
       "mean        0.254812  0.944622  0.669159   0.617241  0.730612\n",
       "median      0.264692  0.971838  0.781925   0.753788  0.812245\n",
       "max         0.422737  0.978555  0.806867   0.850679  0.767347\n",
       "idf-mean    0.437304  0.973403  0.790419   0.773438  0.808163"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "partial-little",
   "metadata": {},
   "source": [
    "Самый слабый результат получили при методе эмбединга mean. \n",
    "Остальные результаты могут быть использованы при разных требованиях заказчика:\n",
    "- Медиану для приоритета recall, то есть когда важно не пропустить целевой класс, пусть и с большим количеством ложно-положительных\n",
    "- Максимальное значение в обратном случае, когда важнее недопустить ложно-положительных\n",
    "- Взвешивание по idf в случае баланса precision и recall"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
